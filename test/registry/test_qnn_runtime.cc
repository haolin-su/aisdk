#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <dlfcn.h>
#include <vector>
#include <unordered_map>
#include <memory>
#include <iostream>
#include <fstream>
#include <cstring>
#include <cstdlib>
#include <iterator>
#include <algorithm>

#include "QnnInterface.h"
#include "QnnContext.h"
#include "QnnGraph.h"
#include "QnnTensor.h"
#include "QnnTypes.h"
#include "System/QnnSystemInterface.h"

enum class StatusCode {
  SUCCESS,
  FAILURE,
  FAIL_LOAD_BACKEND,
  FAIL_LOAD_MODEL,
  FAIL_SYM_FUNCTION,
  FAIL_GET_INTERFACE_PROVIDERS,
  FAIL_LOAD_SYSTEM_LIB,
};

// Graph Related Function Handle Types
typedef enum ModelError {
  MODEL_NO_ERROR               = 0,
  MODEL_TENSOR_ERROR           = 1,
  MODEL_PARAMS_ERROR           = 2,
  MODEL_NODES_ERROR            = 3,
  MODEL_GRAPH_ERROR            = 4,
  MODEL_CONTEXT_ERROR          = 5,
  MODEL_GENERATION_ERROR       = 6,
  MODEL_SETUP_ERROR            = 7,
  MODEL_INVALID_ARGUMENT_ERROR = 8,
  MODEL_FILE_ERROR             = 9,
  MODEL_MEMORY_ALLOCATE_ERROR  = 10,
  // Value selected to ensure 32 bits.
  MODEL_UNKNOWN_ERROR = 0x7FFFFFFF
} ModelError_t;

// typedef struct {
//   /// Integer identifier for a tensor.
//   uint32_t id;
//   /// Tensor name.
//   const char* name;
//   /// Tensor type.
//   Qnn_TensorType_t type;
//   /// Tensor data formatting in memory (refer to definition type for info).
//   Qnn_TensorDataFormat_t dataFormat;
//   /// Tensor data type.
//   Qnn_DataType_t dataType;
//   /// Tensor quantization params.
//   Qnn_QuantizeParams_t quantizeParams;
//   /// Tensor rank.
//   uint32_t rank;
//   /// Tensor dimension array of length _rank_. For detailed behavior of dimensions field with
//   /// various APIs, refer SDK documentation. Must be NULL when rank is 0.
//   uint32_t* dimensions;
//   /// Tensor memory type.
//   Qnn_TensorMemType_t memType;
//   /// Actual data contained in the tensor.
//   union UNNAMED {
//     /// Tensor data provided by client as a pointer to raw memory (see QNN_TENSORMEMTYPE_RAW).
//     Qnn_ClientBuffer_t clientBuf;
//     /// Tensor data shared via a memory handle (see QNN_TENSORMEMTYPE_MEMHANDLE).
//     Qnn_MemHandle_t memHandle;
//   };
// } Qnn_TensorV1_t;
// typedef struct {
//   /// Unique integer identifier for a tensor, generated by the backend based on the tensor name.
//   uint32_t id;
//   /// Unique tensor name.
//   const char* name;
//   /// Tensor type.
//   Qnn_TensorType_t type;
//   /// Tensor data formatting in memory (refer to definition type for info).
//   Qnn_TensorDataFormat_t dataFormat;
//   /// Tensor data type.
//   Qnn_DataType_t dataType;
//   /// Tensor quantization params.
//   Qnn_QuantizeParams_t quantizeParams;
//   /// Tensor rank. Note that rank cannot be dynamic.
//   uint32_t rank;
//   /// Tensor dimension array of length _rank_. For detailed behavior of dimensions field with
//   /// various APIs, refer to their API documentation. Must be NULL when rank is 0. Must contain
//   /// non-zero values if non-null.
//   uint32_t* dimensions;
//   /// Tensor memory type.
//   Qnn_TensorMemType_t memType;
//   /// Actual data contained in the tensor.
//   union UNNAMED {
//     /// Tensor data provided by client as a pointer to raw memory (see QNN_TENSORMEMTYPE_RAW).
//     Qnn_ClientBuffer_t clientBuf;
//     /// Tensor data shared via a memory handle (see QNN_TENSORMEMTYPE_MEMHANDLE).
//     Qnn_MemHandle_t memHandle;
//   };
//   /// A boolean array of length _rank_ indicating if a tensor dimension is dynamic. Must be NULL
//   /// when rank is 0. Can be NULL if all dimensions are static. A true (non-zero) value indicates
//   /// the corresponding dimension is dynamic and a false (zero) value indicates the corresponding
//   /// dimension is static. Note that QNN_TENSOR_TYPE_STATIC tensors (see _type_) cannot have dynamic
//   /// dimensions. Support for this field can be queried via
//   /// QNN_PROPERTY_TENSOR_SUPPORT_DYNAMIC_DIMENSIONS. If this field is unsupported, it must be NULL.
//   uint8_t* isDynamicDimensions;
//   /// Sparse tensor parameters. Pertains only to sparse tensors (see QNN_TENSOR_DATA_FORMAT_SPARSE).
//   /// Support for this field can be queried via QNN_PROPERTY_TENSOR_SUPPORT_SPARSITY.
//   Qnn_SparseParams_t sparseParams;
//   /// Indicates whether or not a call to QnnGraph_execute[Async] produced this output tensor.
//   /// Applicable only to QNN_TENSOR_TYPE_APP_READ and QNN_TENSOR_TYPE_APP_READWRITE tensor types.
//   /// This field will be undefined if QNN_PROPERTY_GRAPH_SUPPORT_EARLY_TERMINATION is not
//   /// supported. Otherwise, this field is not used.
//   uint8_t isProduced;
// } Qnn_TensorV2_t;
// typedef struct {
//   /// Version of the QNN tensor
//   Qnn_TensorVersion_t version;
//   union UNNAMED {
//     /// Tensor version 1 (see QNN_TENSOR_VERSION_1)
//     Qnn_TensorV1_t v1;
//     /// Tensor version 2 (see QNN_TENSOR_VERSION_2)
//     Qnn_TensorV2_t v2;
//   };
// } Qnn_Tensor_t;

typedef struct GraphInfo {
  Qnn_GraphHandle_t graph;
  char *graphName;
  Qnn_Tensor_t *inputTensors;
  uint32_t numInputTensors;
  Qnn_Tensor_t *outputTensors;
  uint32_t numOutputTensors;
} GraphInfo_t;
typedef GraphInfo_t *GraphInfoPtr_t;

typedef struct GraphConfigInfo {
  char *graphName;
  const QnnGraph_Config_t **graphConfigs;
} GraphConfigInfo_t;

typedef ModelError_t (*ComposeGraphsFnHandleType_t)(
    Qnn_BackendHandle_t,
    QNN_INTERFACE_VER_TYPE,
    Qnn_ContextHandle_t,
    const GraphConfigInfo_t **,
    const uint32_t,
    GraphInfo_t ***,
    uint32_t *,
    bool,
    QnnLog_Callback_t,
    QnnLog_Level_t);

typedef ModelError_t (*FreeGraphInfoFnHandleType_t)(
    GraphInfo_t ***, uint32_t);

typedef struct QnnFunctionPointers {
    ComposeGraphsFnHandleType_t composeGraphsFnHandle;
    FreeGraphInfoFnHandleType_t freeGraphInfoFnHandle;
    QNN_INTERFACE_VER_TYPE qnnInterface;
    QNN_SYSTEM_INTERFACE_VER_TYPE qnnSystemInterface;
} QnnFunctionPointers;

void logStdoutCallback(const char* fmt,
                           QnnLog_Level_t level,
                           uint64_t timestamp,
                           va_list argp) {
      const char* levelStr = "";
      switch (level) {
      case QNN_LOG_LEVEL_ERROR:
        levelStr = " ERROR ";
        break;
      case QNN_LOG_LEVEL_WARN:
        levelStr = "WARNING";
        break;
      case QNN_LOG_LEVEL_INFO:
        levelStr = "  INFO ";
        break;
      case QNN_LOG_LEVEL_DEBUG:
        levelStr = " DEBUG ";
        break;
      case QNN_LOG_LEVEL_VERBOSE:
        levelStr = "VERBOSE";
        break;
      case QNN_LOG_LEVEL_MAX:
        levelStr = "UNKNOWN";
        break;
      }
    //   fprintf(stdout, "%8.1fms [%-7s] ", ms, levelStr);
      vfprintf(stdout, fmt, argp);
      fprintf(stdout, "\n");
}

template <class T>
static inline T resolveSymbol(void* libHandle, const char* symName) {
    T ptr = (T)::dlsym(libHandle, symName);
    if (ptr == nullptr) {
        printf("Unable to access symbol [%s]. dlerror(): %s", symName, dlerror());
    }
    return ptr;
}

size_t getFileSize(std::string filePath) {
  std::ifstream in(filePath, std::ifstream::binary);
  if (!in) {
    printf("Failed to open input file: %s\n", filePath.c_str());
    return 0;
  }
  in.seekg(0, in.end);
  const size_t length = in.tellg();
  in.seekg(0, in.beg);
  return  length;
}


inline uint32_t getQnnTensorId(const Qnn_Tensor_t& tensor) {
  // TensorCompatTest justifies no need to check version
  return tensor.v1.id;
}

inline uint32_t getQnnTensorId(const Qnn_Tensor_t* const tensor) { return getQnnTensorId(*tensor); }

inline const char* getQnnTensorName(const Qnn_Tensor_t& tensor) {
  // TensorCompatTest justifies no need to check version
  return tensor.v1.name;
}

inline const char* getQnnTensorName(const Qnn_Tensor_t* const tensor) {
  return getQnnTensorName(*tensor);
}

inline Qnn_TensorType_t getQnnTensorType(const Qnn_Tensor_t& tensor) {
  // TensorCompatTest justifies no need to check version
  return tensor.v1.type;
}

inline Qnn_TensorType_t getQnnTensorType(const Qnn_Tensor_t* const tensor) {
  return getQnnTensorType(*tensor);
}

inline Qnn_TensorDataFormat_t getQnnTensorDataFormat(const Qnn_Tensor_t& tensor) {
  // TensorCompatTest justifies no need to check version
  return tensor.v1.dataFormat;
}

inline Qnn_TensorDataFormat_t getQnnTensorDataFormat(const Qnn_Tensor_t* const tensor) {
  return getQnnTensorDataFormat(*tensor);
}

inline Qnn_DataType_t getQnnTensorDataType(const Qnn_Tensor_t& tensor) {
  // TensorCompatTest justifies no need to check version
  return tensor.v1.dataType;
}

inline Qnn_DataType_t getQnnTensorDataType(const Qnn_Tensor_t* const tensor) {
  return getQnnTensorDataType(*tensor);
}

inline Qnn_QuantizeParams_t getQnnTensorQuantParams(const Qnn_Tensor_t& tensor) {
  // TensorCompatTest justifies no need to check version
  return tensor.v1.quantizeParams;
}

inline Qnn_QuantizeParams_t getQnnTensorQuantParams(const Qnn_Tensor_t* const tensor) {
  if (tensor != nullptr) {
    return getQnnTensorQuantParams(*tensor);
  }
  return QNN_QUANTIZE_PARAMS_INIT;
}

inline uint32_t getQnnTensorRank(const Qnn_Tensor_t& tensor) {
  // TensorCompatTest justifies no need to check version
  return tensor.v1.rank;
}

inline uint32_t getQnnTensorRank(const Qnn_Tensor_t* const tensor) {
  if (tensor != nullptr) {
    return getQnnTensorRank(*tensor);
  }
  return 0u;
}

inline uint32_t* getQnnTensorDimensions(const Qnn_Tensor_t& tensor) {
  // TensorCompatTest justifies no need to check version
  return tensor.v1.dimensions;
}

inline uint32_t* getQnnTensorDimensions(const Qnn_Tensor_t* const tensor) {
  return getQnnTensorDimensions(*tensor);
}

inline uint8_t* getQnnTensorIsDynamicDimensions(const Qnn_Tensor_t& tensor) {
  if (tensor.version == QNN_TENSOR_VERSION_1) {
    return NULL;
  } else if (tensor.version == QNN_TENSOR_VERSION_2) {
    return tensor.v2.isDynamicDimensions;
  }
  return NULL;
}

inline uint8_t* getQnnTensorIsDynamicDimensions(const Qnn_Tensor_t* tensor) {
  return getQnnTensorIsDynamicDimensions(*tensor);
}

inline Qnn_SparseParams_t getQnnTensorSparseParams(const Qnn_Tensor_t& tensor) {
  if (tensor.version == QNN_TENSOR_VERSION_1) {
    return QNN_SPARSE_PARAMS_INIT;
  } else if (tensor.version == QNN_TENSOR_VERSION_2) {
    return tensor.v2.sparseParams;
  }
  return QNN_SPARSE_PARAMS_INIT;
}

inline Qnn_SparseParams_t getQnnTensorSparseParams(const Qnn_Tensor_t* tensor) {
  return getQnnTensorSparseParams(*tensor);
}

inline Qnn_TensorMemType_t getQnnTensorMemType(const Qnn_Tensor_t& tensor) {
  // TensorCompatTest justifies no need to check version
  return tensor.v1.memType;
}

inline Qnn_TensorMemType_t getQnnTensorMemType(const Qnn_Tensor_t* const tensor) {
  return getQnnTensorMemType(*tensor);
}

inline Qnn_ClientBuffer_t getQnnTensorClientBuf(const Qnn_Tensor_t& tensor) {
  // TensorCompatTest justifies no need to check version
  return tensor.v1.clientBuf;
}

inline Qnn_ClientBuffer_t getQnnTensorClientBuf(const Qnn_Tensor_t* const tensor) {
  return getQnnTensorClientBuf(*tensor);
}

inline Qnn_MemHandle_t getQnnTensorMemHandle(const Qnn_Tensor_t& tensor) {
  // TensorCompatTest justifies no need to check version
  return tensor.v1.memHandle;
}

inline Qnn_MemHandle_t getQnnTensorMemHandle(const Qnn_Tensor_t* const tensor) {
  return getQnnTensorMemHandle(*tensor);
}

inline void setQnnTensorId(Qnn_Tensor_t& tensor, const uint32_t id) {
  // TensorCompatTest justifies no need to check version
  tensor.v1.id = id;
}

inline void setQnnTensorId(Qnn_Tensor_t* const tensor, const uint32_t id) {
  setQnnTensorId(*tensor, id);
}

inline void setQnnTensorName(Qnn_Tensor_t& tensor, const char* const name) {
  // TensorCompatTest justifies no need to check version
  tensor.v1.name = name;
}

inline void setQnnTensorName(Qnn_Tensor_t* const tensor, const char* const name) {
  setQnnTensorName(*tensor, name);
}

inline void setQnnTensorType(Qnn_Tensor_t& tensor, const Qnn_TensorType_t type) {
  // TensorCompatTest justifies no need to check version
  tensor.v1.type = type;
}

inline void setQnnTensorType(Qnn_Tensor_t* const tensor, const Qnn_TensorType_t type) {
  setQnnTensorType(*tensor, type);
}

inline void setQnnTensorDataFormat(Qnn_Tensor_t& tensor, const Qnn_TensorDataFormat_t dataFormat) {
  // TensorCompatTest justifies no need to check version
  tensor.v1.dataFormat = dataFormat;
}

inline void setQnnTensorDataFormat(Qnn_Tensor_t* const tensor,
                                   const Qnn_TensorDataFormat_t format) {
  setQnnTensorDataFormat(*tensor, format);
}

inline void setQnnTensorDataType(Qnn_Tensor_t& tensor, const Qnn_DataType_t dataType) {
  // TensorCompatTest justifies no need to check version
  tensor.v1.dataType = dataType;
}

inline void setQnnTensorDataType(Qnn_Tensor_t* const tensor, const Qnn_DataType_t dataType) {
  setQnnTensorDataType(*tensor, dataType);
}

inline void setQnnTensorQuantParams(Qnn_Tensor_t& tensor,
                                    const Qnn_QuantizeParams_t quantizeParams) {
  // TensorCompatTest justifies no need to check version
  tensor.v1.quantizeParams = quantizeParams;
}

inline void setQnnTensorQuantParams(Qnn_Tensor_t* const tensor, const Qnn_QuantizeParams_t params) {
  setQnnTensorQuantParams(*tensor, params);
}

inline void setQnnTensorRank(Qnn_Tensor_t& tensor, const uint32_t rank) {
  // TensorCompatTest justifies no need to check version
  tensor.v1.rank = rank;
}

inline void setQnnTensorRank(Qnn_Tensor_t* const tensor, const uint32_t rank) {
  setQnnTensorRank(*tensor, rank);
}

inline void setQnnTensorDimensions(Qnn_Tensor_t& tensor, uint32_t* const dimensions) {
  // TensorCompatTest justifies no need to check version
  tensor.v1.dimensions = dimensions;
}

inline void setQnnTensorDimensions(Qnn_Tensor_t* const tensor, uint32_t* const dimensions) {
  setQnnTensorDimensions(*tensor, dimensions);
}

inline void setQnnTensorIsDynamicDimensions(Qnn_Tensor_t& tensor,
                                            uint8_t* const isDynamicDimensions) {
  if (tensor.version == QNN_TENSOR_VERSION_2) {
    tensor.v2.isDynamicDimensions = isDynamicDimensions;
  }
}

inline void setQnnTensorIsDynamicDimensions(Qnn_Tensor_t* tensor,
                                            uint8_t* const isDynamicDimensions) {
  setQnnTensorIsDynamicDimensions(*tensor, isDynamicDimensions);
}

inline void setQnnTensorSparseParams(Qnn_Tensor_t& tensor, const Qnn_SparseParams_t sparseParams) {
  if (tensor.version == QNN_TENSOR_VERSION_2) {
    tensor.v2.sparseParams = sparseParams;
  }
}

inline void setQnnTensorSparseParams(Qnn_Tensor_t* tensor, Qnn_SparseParams_t sparseParams) {
  setQnnTensorSparseParams(*tensor, sparseParams);
}

inline void setQnnTensorMemType(Qnn_Tensor_t& tensor, const Qnn_TensorMemType_t memType) {
  // TensorCompatTest justifies no need to check version
  tensor.v1.memType = memType;
}

inline void setQnnTensorMemType(Qnn_Tensor_t* const tensor, const Qnn_TensorMemType_t memType) {
  setQnnTensorMemType(*tensor, memType);
}

inline void setQnnTensorClientBuf(Qnn_Tensor_t& tensor, const Qnn_ClientBuffer_t clientBuf) {
  // TensorCompatTest justifies no need to check version
  tensor.v1.clientBuf = clientBuf;
}

inline void setQnnTensorClientBuf(Qnn_Tensor_t* const tensor, const Qnn_ClientBuffer_t clientBuf) {
  setQnnTensorClientBuf(*tensor, clientBuf);
}

inline void setQnnTensorMemHandle(Qnn_Tensor_t& tensor, const Qnn_MemHandle_t memHandle) {
  // TensorCompatTest justifies no need to check version
  tensor.v1.memHandle = memHandle;
}

inline void setQnnTensorMemHandle(Qnn_Tensor_t* const tensor, const Qnn_MemHandle_t handle) {
  setQnnTensorMemHandle(*tensor, handle);
}


// Accessors for QNN Tensor
#define QNN_TENSOR_GET_ID(tensor)                    getQnnTensorId(tensor)
#define QNN_TENSOR_GET_NAME(tensor)                  getQnnTensorName(tensor)
#define QNN_TENSOR_GET_TYPE(tensor)                  getQnnTensorType(tensor)
#define QNN_TENSOR_GET_DATA_FORMAT(tensor)           getQnnTensorDataFormat(tensor)
#define QNN_TENSOR_GET_DATA_TYPE(tensor)             getQnnTensorDataType(tensor)
#define QNN_TENSOR_GET_QUANT_PARAMS(tensor)          getQnnTensorQuantParams(tensor)
#define QNN_TENSOR_GET_RANK(tensor)                  getQnnTensorRank(tensor)
#define QNN_TENSOR_GET_DIMENSIONS(tensor)            getQnnTensorDimensions(tensor)
#define QNN_TENSOR_GET_IS_DYNAMIC_DIMENSIONS(tensor) getQnnTensorIsDynamicDimensions(tensor)
#define QNN_TENSOR_GET_SPARSE_PARAMS(tensor)         getQnnTensorSparseParams(tensor)
#define QNN_TENSOR_GET_MEM_TYPE(tensor)              getQnnTensorMemType(tensor)
#define QNN_TENSOR_GET_CLIENT_BUF(tensor)            getQnnTensorClientBuf(tensor)
#define QNN_TENSOR_GET_MEM_HANDLE(tensor)            getQnnTensorMemHandle(tensor)

// Modifiers for QNN Tensor
#define QNN_TENSOR_SET_ID(tensor, value)           setQnnTensorId(tensor, value)
#define QNN_TENSOR_SET_NAME(tensor, value)         setQnnTensorName(tensor, value)
#define QNN_TENSOR_SET_TYPE(tensor, value)         setQnnTensorType(tensor, value)
#define QNN_TENSOR_SET_DATA_FORMAT(tensor, value)  setQnnTensorDataFormat(tensor, value)
#define QNN_TENSOR_SET_DATA_TYPE(tensor, value)    setQnnTensorDataType(tensor, value)
#define QNN_TENSOR_SET_QUANT_PARAMS(tensor, value) setQnnTensorQuantParams(tensor, value)
#define QNN_TENSOR_SET_RANK(tensor, value)         setQnnTensorRank(tensor, value)
#define QNN_TENSOR_SET_DIMENSIONS(tensor, value)   setQnnTensorDimensions(tensor, value)
#define QNN_TENSOR_SET_IS_DYNAMIC_DIMENSIONS(tensor, value) \
  setQnnTensorIsDynamicDimensions(tensor, value)
#define QNN_TENSOR_SET_SPARSE_PARAMS(tensor, value) setQnnTensorSparseParams(tensor, value)
#define QNN_TENSOR_SET_MEM_TYPE(tensor, value)      setQnnTensorMemType(tensor, value)
#define QNN_TENSOR_SET_CLIENT_BUF(tensor, value)    setQnnTensorClientBuf(tensor, value)
#define QNN_TENSOR_SET_MEM_HANDLE(tensor, value)    setQnnTensorMemHandle(tensor, value)


int freeQnnTensor(Qnn_Tensor_t &tensor) {
  // free all pointer allocations in struct
  free((void *)QNN_TENSOR_GET_NAME(tensor));
  free(QNN_TENSOR_GET_DIMENSIONS(tensor));
  if (QNN_TENSOR_GET_IS_DYNAMIC_DIMENSIONS(tensor)) {
    free(QNN_TENSOR_GET_IS_DYNAMIC_DIMENSIONS(tensor));
  }
  return MODEL_NO_ERROR;
}

int freeQnnTensors(Qnn_Tensor_t *&tensors,
                                                              uint32_t numTensors) {
  // free all pointer allocations in struct
  for (size_t i = 0; i < numTensors; i++) {
    freeQnnTensor(tensors[i]);
  }
  free(tensors);
  return MODEL_NO_ERROR;
}

int freeGraphsInfo(GraphInfoPtr_t **graphsInfo,
                                                              uint32_t numGraphs) {
  if (graphsInfo == nullptr || *graphsInfo == nullptr) {
    return MODEL_TENSOR_ERROR;
  }
  for (uint32_t i = 0; i < numGraphs; i++) {
    free((*graphsInfo)[i]->graphName);
    freeQnnTensors((*graphsInfo)[i]->inputTensors, (*graphsInfo)[i]->numInputTensors);
    freeQnnTensors((*graphsInfo)[i]->outputTensors, (*graphsInfo)[i]->numOutputTensors);
  }
  free(**graphsInfo);
  free(*graphsInfo);
  *graphsInfo = nullptr;
  return MODEL_NO_ERROR;
}

int memscpy(void *dst, size_t dstSize, const void *src, size_t copySize) {
  if (!dst || !src || !dstSize || !copySize) return 0;

  size_t minSize = dstSize < copySize ? dstSize : copySize;

  memcpy(dst, src, minSize);

  return minSize;
}



bool deepCopyQnnTensorInfo(Qnn_Tensor_t *dst, const Qnn_Tensor_t *src) {
  if (nullptr == dst || nullptr == src) {
    printf("Received nullptr");
    return -1;
  }
  // set tensor.version before using QNN_TENSOR_SET macros, as they require the version to be set
  // to correctly assign values
  dst->version           = src->version;
  const char *tensorName = QNN_TENSOR_GET_NAME(src);
  if (!tensorName) {
    QNN_TENSOR_SET_NAME(dst, nullptr);
  } else {
    QNN_TENSOR_SET_NAME(dst, strndup(tensorName, strlen(tensorName)));
  }
  QNN_TENSOR_SET_ID(dst, QNN_TENSOR_GET_ID(src));
  QNN_TENSOR_SET_TYPE(dst, QNN_TENSOR_GET_TYPE(src));
  QNN_TENSOR_SET_DATA_FORMAT(dst, QNN_TENSOR_GET_DATA_FORMAT(src));
  QNN_TENSOR_SET_DATA_TYPE(dst, QNN_TENSOR_GET_DATA_TYPE(src));
  Qnn_QuantizeParams_t qParams = QNN_QUANTIZE_PARAMS_INIT;
  qParams.encodingDefinition   = QNN_TENSOR_GET_QUANT_PARAMS(src).encodingDefinition;
  qParams.quantizationEncoding = QNN_QUANTIZATION_ENCODING_UNDEFINED;
  if (QNN_TENSOR_GET_QUANT_PARAMS(src).quantizationEncoding ==
      QNN_QUANTIZATION_ENCODING_SCALE_OFFSET) {
    qParams.quantizationEncoding = QNN_TENSOR_GET_QUANT_PARAMS(src).quantizationEncoding;
    qParams.scaleOffsetEncoding  = QNN_TENSOR_GET_QUANT_PARAMS(src).scaleOffsetEncoding;
  } else if (QNN_TENSOR_GET_QUANT_PARAMS(src).quantizationEncoding ==
             QNN_QUANTIZATION_ENCODING_AXIS_SCALE_OFFSET) {
    qParams.quantizationEncoding = QNN_TENSOR_GET_QUANT_PARAMS(src).quantizationEncoding;
    qParams.axisScaleOffsetEncoding.axis =
        QNN_TENSOR_GET_QUANT_PARAMS(src).axisScaleOffsetEncoding.axis;
    qParams.axisScaleOffsetEncoding.numScaleOffsets =
        QNN_TENSOR_GET_QUANT_PARAMS(src).axisScaleOffsetEncoding.numScaleOffsets;
    if (QNN_TENSOR_GET_QUANT_PARAMS(src).axisScaleOffsetEncoding.numScaleOffsets > 0) {
      qParams.axisScaleOffsetEncoding.scaleOffset = (Qnn_ScaleOffset_t *)malloc(
          QNN_TENSOR_GET_QUANT_PARAMS(src).axisScaleOffsetEncoding.numScaleOffsets *
          sizeof(Qnn_ScaleOffset_t));
      if (qParams.axisScaleOffsetEncoding.scaleOffset) {
        for (size_t idx = 0;
             idx < QNN_TENSOR_GET_QUANT_PARAMS(src).axisScaleOffsetEncoding.numScaleOffsets;
             idx++) {
          qParams.axisScaleOffsetEncoding.scaleOffset[idx].scale =
              QNN_TENSOR_GET_QUANT_PARAMS(src).axisScaleOffsetEncoding.scaleOffset[idx].scale;
          qParams.axisScaleOffsetEncoding.scaleOffset[idx].offset =
              QNN_TENSOR_GET_QUANT_PARAMS(src).axisScaleOffsetEncoding.scaleOffset[idx].offset;
        }
      }
    }
  }
  QNN_TENSOR_SET_QUANT_PARAMS(dst, qParams);
  QNN_TENSOR_SET_RANK(dst, QNN_TENSOR_GET_RANK(src));
  QNN_TENSOR_SET_DIMENSIONS(dst, nullptr);
  if (QNN_TENSOR_GET_RANK(src) > 0) {
    QNN_TENSOR_SET_DIMENSIONS(dst, (uint32_t *)malloc(QNN_TENSOR_GET_RANK(src) * sizeof(uint32_t)));
    if (QNN_TENSOR_GET_DIMENSIONS(dst)) {
      memscpy(QNN_TENSOR_GET_DIMENSIONS(dst),
                             QNN_TENSOR_GET_RANK(src) * sizeof(uint32_t),
                             QNN_TENSOR_GET_DIMENSIONS(src),
                             QNN_TENSOR_GET_RANK(src) * sizeof(uint32_t));
    }
    if (QNN_TENSOR_GET_IS_DYNAMIC_DIMENSIONS(src)) {
      QNN_TENSOR_SET_IS_DYNAMIC_DIMENSIONS(
          dst, (uint8_t *)malloc(QNN_TENSOR_GET_RANK(src) * sizeof(uint8_t)));
      memscpy(QNN_TENSOR_GET_IS_DYNAMIC_DIMENSIONS(dst),
                             QNN_TENSOR_GET_RANK(src) * sizeof(uint8_t),
                             QNN_TENSOR_GET_IS_DYNAMIC_DIMENSIONS(src),
                             QNN_TENSOR_GET_RANK(src) * sizeof(uint8_t));
    }
  }
  QNN_TENSOR_SET_SPARSE_PARAMS(dst, QNN_TENSOR_GET_SPARSE_PARAMS(src));
  return 0;
}

bool copyTensorsInfo(const Qnn_Tensor_t *tensorsInfoSrc,
                                 Qnn_Tensor_t *&tensorWrappers,
                                 uint32_t tensorsCount) {
  
  auto returnStatus = true;
  tensorWrappers    = (Qnn_Tensor_t *)calloc(tensorsCount, sizeof(Qnn_Tensor_t));
  if (nullptr == tensorWrappers) {
    printf("Failed to allocate memory for tensorWrappers.");
    return -1;
  }
  if (returnStatus) {
    for (size_t tIdx = 0; tIdx < tensorsCount; tIdx++) {
      printf("Extracting tensorInfo for tensor Idx: %ld\n", tIdx);
      tensorWrappers[tIdx] = QNN_TENSOR_INIT;
      deepCopyQnnTensorInfo(&tensorWrappers[tIdx], &tensorsInfoSrc[tIdx]);
    }
  }
  
  return returnStatus;
}

bool copyGraphsInfoV1(const QnnSystemContext_GraphInfoV1_t *graphInfoSrc,
                                  GraphInfo_t *graphInfoDst) {
  graphInfoDst->graphName = nullptr;
  if (graphInfoSrc->graphName) {
    graphInfoDst->graphName =
        strndup(graphInfoSrc->graphName, strlen(graphInfoSrc->graphName));
  }
  graphInfoDst->inputTensors    = nullptr;
  graphInfoDst->numInputTensors = 0;
  if (graphInfoSrc->graphInputs) {
    if (!copyTensorsInfo(
            graphInfoSrc->graphInputs, graphInfoDst->inputTensors, graphInfoSrc->numGraphInputs)) {
      return -1;
    }
    graphInfoDst->numInputTensors = graphInfoSrc->numGraphInputs;
  }
  graphInfoDst->outputTensors    = nullptr;
  graphInfoDst->numOutputTensors = 0;
  if (graphInfoSrc->graphOutputs) {
    if (!copyTensorsInfo(graphInfoSrc->graphOutputs,
                         graphInfoDst->outputTensors,
                         graphInfoSrc->numGraphOutputs)) {
      return -1;
    }
    graphInfoDst->numOutputTensors = graphInfoSrc->numGraphOutputs;
  }
  return 0;
}

bool copyGraphsInfo(const QnnSystemContext_GraphInfo_t *graphsInput,
                                const uint32_t numGraphs,
                                GraphInfo_t **&graphsInfo) {
  
  if (!graphsInput) {
    printf("Received nullptr for graphsInput.");
    return -1;
  }
  auto returnStatus = true;
  graphsInfo =
      (GraphInfo_t **)calloc(numGraphs, sizeof(GraphInfo_t *));
  GraphInfo_t *graphInfoArr =
      (GraphInfo_t *)calloc(numGraphs, sizeof(GraphInfo_t));
  if (nullptr == graphsInfo || nullptr == graphInfoArr) {
    printf("Failure to allocate memory for *graphInfo");
    returnStatus = false;
  }
  if (true == returnStatus) {
    for (size_t gIdx = 0; gIdx < numGraphs; gIdx++) {
      printf("Extracting graphsInfo for graph Idx: %ld", gIdx);
      if (graphsInput[gIdx].version == QNN_SYSTEM_CONTEXT_GRAPH_INFO_VERSION_1) {
        copyGraphsInfoV1(&graphsInput[gIdx].graphInfoV1, &graphInfoArr[gIdx]);
      }
      graphsInfo[gIdx] = graphInfoArr + gIdx;
    }
  }
  if (true != returnStatus) {
    printf("Received an ERROR during extractGraphsInfo. Freeing resources.");
    if (graphsInfo) {
      for (uint32_t gIdx = 0; gIdx < numGraphs; gIdx++) {
        if (graphsInfo[gIdx]) {
          if (nullptr != graphsInfo[gIdx]->graphName) {
            free(graphsInfo[gIdx]->graphName);
            graphsInfo[gIdx]->graphName = nullptr;
          }
          freeQnnTensors(graphsInfo[gIdx]->inputTensors,
                                          graphsInfo[gIdx]->numInputTensors);
          freeQnnTensors(graphsInfo[gIdx]->outputTensors,
                                          graphsInfo[gIdx]->numOutputTensors);
        }
      }
      free(*graphsInfo);
    }
    free(graphsInfo);
    graphsInfo = nullptr;
  }
  
  return true;
}

int main(int argc, char* argv[]) {
    // 解析参数
    // 参数一：qnn backend文件路径
    // 参数二：qnn模型文件路径
    // 参数三：模型文件路径
    // 参数四：输入文件路径
    // 参数五：输出文件路径
    // 参数六：运行模式（CPU/GPU）
    // 参数七：是否使用用户提供的输入/输出缓冲区
    // 参数八：qnn系统文件路径
    // 参数九：缓存的模型文件路径

    // 解析参数
    if (argc < 8) {
        printf("Usage: %s <backend_file> <qnn_model_file> <input_file> <output_file> <runtime> <use_user_supplied_buffers>\n", argv[0]);
        return 1;
    }

    const char* qnn_backend_file = argv[1];
    const char* qnn_model_file = argv[2];
    const char* input_file = argv[3];
    const char* output_file = argv[4];
    const char* runtime = argv[5];
    bool use_user_supplied_buffers = (bool)atoi(argv[6]);
    const char* qnn_systefile = argv[7];
    const char* cache_model_file = argv[8];
    // 打印参数
    printf("backend_file: %s\n", qnn_backend_file);
    printf("qnn_model_file: %s\n", qnn_model_file);
    printf("input_file: %s\n", input_file);
    printf("output_file: %s\n", output_file);
    printf("runtime: %s\n", runtime);
    printf("use_user_supplied_buffers: %d\n", use_user_supplied_buffers);
    printf("qnn_systefile: %s\n", qnn_systefile);
    printf("cache_model_file: %s\n", cache_model_file);


//      --backend ${QNN_SDK_ROOT}/lib/x86_64-linux-clang/libQnnCpu.so \
// 4              --model ${QNN_SDK_ROOT}/examples/QNN/example_libs/x86_64-linux-clang/libqnn_model_float.so
    
  
    printf("---------------------Begin to test snpe runtime---------------------\n");

    
    QnnFunctionPointers* qnnFunctionPointers;

    // 1. Loading a backend
    
    // 2. Resolving symbols in shared libraries
    // A generic function to resolve symbols in a library
    

    void* libBackendHandle = dlopen(qnn_backend_file, RTLD_NOW | RTLD_LOCAL);
    if (nullptr == libBackendHandle) {
        printf("Unable to load backend. dlerror(): %s",  dlerror());
        return -1;
    }
    printf("Loaded backend library: %s\n", qnn_backend_file);

    /* Resolve the symbol for Qnn_ErrorHandle_t QnnInterface_getProviders(const QnnInterface_t*** providerList,
                                                                          uint32_t* numProviders)
       API */
    typedef Qnn_ErrorHandle_t (*QnnInterfaceGetProvidersFn_t)(const QnnInterface_t*** providerList,
                                                              uint32_t* numProviders);
    
    
    QnnInterfaceGetProvidersFn_t getInterfaceProviders {nullptr};
    
    getInterfaceProviders = resolveSymbol<QnnInterfaceGetProvidersFn_t>(libBackendHandle, "QnnInterface_getProviders");
    if (nullptr == getInterfaceProviders) {
        return -1;
    }
    printf("Found QnnInterface_getProviders function in backend library.\n");

    // 3. Use QNN Interface to obtain function pointers
    QnnInterface_t** interfaceProviders{nullptr};
    uint32_t numProviders{0};
    // Query for al available interfaces
    if (QNN_SUCCESS !=
       getInterfaceProviders((const QnnInterface_t***)&interfaceProviders, &numProviders)) {
        printf("Failed to get interface providers.");
        return -1;
    }
    // Check for validity of returned interfaces
    if (nullptr == interfaceProviders) {
        printf("Failed to get interface providers: null interface providers received.");
        return -1;
    }
    if (0 == numProviders) {
        printf("Failed to get interface providers: 0 interface providers.");
        return -1;
    }
    printf("Found %d interface providers in the backend library.\n", numProviders);

    bool foundValidInterface{false};
    // Loop through all available interface providers and pick the one that suits the current API
    // version
    for (size_t pIdx = 0; pIdx < numProviders; pIdx++) {
        if (QNN_API_VERSION_MAJOR == interfaceProviders[pIdx]->apiVersion.coreApiVersion.major && \
            QNN_API_VERSION_MINOR <= interfaceProviders[pIdx]->apiVersion.coreApiVersion.minor) {
            foundValidInterface                 = true;
            qnnFunctionPointers = new QnnFunctionPointers();
            qnnFunctionPointers->qnnInterface = interfaceProviders[pIdx]->QNN_INTERFACE_VER_NAME;
            printf("Found a valid interface, major version: %d, minor version: %d\n",
                    interfaceProviders[pIdx]->apiVersion.coreApiVersion.major, interfaceProviders[pIdx]->apiVersion.coreApiVersion.minor);
            break;
        }
    }
    if (!foundValidInterface) {
        printf("Unable to find a valid interface.");
        libBackendHandle = nullptr;
        return -1;
    }
    printf("Found a valid interface\n");
    
    // load model相关的接口
     void* libModelHandle = dlopen(qnn_model_file, RTLD_NOW | RTLD_LOCAL);

    if (nullptr == libModelHandle) {
        printf("Unable to load model. dlerror(): %s", dlerror());
        return -1;
    }
    printf("Loaded model library: %s\n", qnn_model_file);
    
    std::string modelPrepareFunc = "QnnModel_composeGraphs";
    qnnFunctionPointers->composeGraphsFnHandle =
        resolveSymbol<ComposeGraphsFnHandleType_t>(libModelHandle,
                                                               modelPrepareFunc.c_str());
    if (nullptr == qnnFunctionPointers->composeGraphsFnHandle) {
        return -1;
    }
    printf("Found QnnModel_composeGraphs function in model library.\n");

    std::string modelFreeFunc = "QnnModel_freeGraphsInfo";
    qnnFunctionPointers->freeGraphInfoFnHandle =
        resolveSymbol<FreeGraphInfoFnHandleType_t>(libModelHandle,
                                                               modelFreeFunc.c_str());
    if (nullptr == qnnFunctionPointers->freeGraphInfoFnHandle) {
        return -1;
    }
    printf("Found QnnModel_freeGraphsInfo function in model library.\n");

    #if 1
    // QNN System Interface can be used to resolve all symbols related to QNN System APIs as shown below
    void* systemLibraryHandle = dlopen(qnn_systefile, RTLD_NOW | RTLD_LOCAL);
    if (nullptr == systemLibraryHandle) {
        printf("Unable to load system library. dlerror(): %s", dlerror());
        return -1;
    }

    typedef Qnn_ErrorHandle_t (*QnnSystemInterfaceGetProvidersFn_t)(
    const QnnSystemInterface_t*** providerList, uint32_t* numProviders);

    QnnSystemInterfaceGetProvidersFn_t getSystemInterfaceProviders{nullptr};
    getSystemInterfaceProviders = resolveSymbol<QnnSystemInterfaceGetProvidersFn_t>(
            systemLibraryHandle, "QnnSystemInterface_getProviders");

    if (nullptr == getSystemInterfaceProviders) {
        return -1;
    }


    QnnSystemInterface_t** systemInterfaceProviders{nullptr};
    
    if (QNN_SUCCESS != getSystemInterfaceProviders(
                         (const QnnSystemInterface_t***)&systemInterfaceProviders, &numProviders)) {
        printf("Failed to get system interface providers.");
        return -1;
    }

    if (nullptr == systemInterfaceProviders) {
        printf("Failed to get system interface providers: null interface providers received.");
        return -1;
    }

    if (0 == numProviders) {
        printf("Failed to get interface providers: 0 interface providers.");
        return -1;
    }

    bool foundValidSystemInterface{false};
    for (size_t pIdx = 0; pIdx < numProviders; pIdx++) {
        if (QNN_SYSTEM_API_VERSION_MAJOR == systemInterfaceProviders[pIdx]->systemApiVersion.major &&
            QNN_SYSTEM_API_VERSION_MINOR <= systemInterfaceProviders[pIdx]->systemApiVersion.minor) {
            foundValidSystemInterface = true;
            qnnFunctionPointers->qnnSystemInterface =
                systemInterfaceProviders[pIdx]->QNN_SYSTEM_INTERFACE_VER_NAME;
            printf("Found a valid system interface, major version: %d, minor version: %d",  
                   systemInterfaceProviders[pIdx]->systemApiVersion.major, systemInterfaceProviders[pIdx]->systemApiVersion.minor);
            break;
        }
    }
    #endif
    
    // 4. Set Up logging 
     Qnn_LogHandle_t logHandle;
    if (QNN_SUCCESS !=
          qnnFunctionPointers->qnnInterface.logCreate(logStdoutCallback, QNN_LOG_LEVEL_DEBUG, &logHandle)) {
        printf("Unable to initialize logging in the backend.");
        return -1;
    }
    
    // 5. Init backend
    Qnn_BackendHandle_t backendHandle;
    // const QnnBackend_Config_t* backendConfigs;
    QnnBackend_Config_t **backendConfigs = nullptr;
    /* Set up any necessary backend configurations */
    if (QNN_BACKEND_NO_ERROR != qnnFunctionPointers->qnnInterface.backendCreate(logHandle,
                                                                                 (const QnnBackend_Config_t**)backendConfigs,
                                                                                 &backendHandle)) {
        printf("Could not initialize backend");
        return -1;
    }

    // 6. Init profiling
    Qnn_ProfileHandle_t profileBackendHandle;
    if (QNN_PROFILE_NO_ERROR != qnnFunctionPointers->qnnInterface.profileCreate(
                                      backendHandle, QNN_PROFILE_LEVEL_BASIC, &profileBackendHandle)) {
        printf("Unable to create profile handle in the backend.");
        return -1;
    }

    // 7. create device
    auto qnnStatus = qnnFunctionPointers->qnnInterface.propertyHasCapability(QNN_PROPERTY_GROUP_DEVICE);
    if (QNN_PROPERTY_NOT_SUPPORTED == qnnStatus) {
        printf("Device property is not supported\n");
    }
    Qnn_DeviceHandle_t deviceHandle {nullptr};
    QnnDevice_Config_t devConfig;
    // devConfig.option = QNN_DEVICE_OPTION_RUNTIME;
        const QnnDevice_Config_t* devConfigArray[] = {&devConfig, nullptr};
    if (QNN_PROPERTY_ERROR_UNKNOWN_KEY == qnnStatus) {
        printf("Backend does not support device creation. Status: %ld", qnnStatus);
        return -1;
    } else {
        Qnn_ErrorHandle_t qnnStatus = qnnFunctionPointers->qnnInterface.deviceCreate(logHandle, devConfigArray, &deviceHandle);
        if (QNN_SUCCESS != qnnStatus && QNN_DEVICE_ERROR_UNSUPPORTED_FEATURE != qnnStatus) {
            printf("Failed to create device: %ld", qnnStatus);
            return -1;
        }
    }
    printf("Created device\n");

    // 8. register op package
    // uint32_t opPackageCount;
    // char* opPackagePath[opPackageCount];
    // char* opPackageInterfaceProvider[opPackageCount];
    // /* Set up required op package paths and interface providers as necessary */
    // for(uint32_t idx = 0; idx < opPackageCount; idx++) {
    //     if (QNN_BACKEND_NO_ERROR !=
    //         qnnFunctionPointers->qnnInterface.backendRegisterOpPackage(backendHandle,
    //                                                                     opPackagePath[idx],
    //                                                                     opPackageInterfaceProvider[idx])) {
    //         printf("Could not register Op Package: %s and interface provider: %s",
    //                 opPackagePath[idx],
    //                 opPackageInterfaceProvider[idx]);
    //         return -1;
    //     }
    // }

    
    

    
    // 检查cache_model_file是否存在
    bool used_cache = false;    
    std::string cachedBinaryPath(cache_model_file);
    if (!std::ifstream(cachedBinaryPath).good()) {
        used_cache = false;    
    } else {
        used_cache = true   ;
    }
    printf("used_cache: %d\n", used_cache);
    // 9. create context
    // const QnnContext_Config_t* contextConfigs;
    QnnContext_Config_t **contextConfigs = nullptr;
    Qnn_ContextHandle_t context;
    GraphInfo_t** graphsInfo;
    uint32_t graphsCount {0};
    if (!used_cache){
        
        /* Set up any context configs that are necessary */
        if (QNN_CONTEXT_NO_ERROR !=
            qnnFunctionPointers->qnnInterface.contextCreate(backendHandle,
                                                            deviceHandle,
                                                            (const QnnContext_Config_t**)contextConfigs,
                                                            &context)) {
            printf("Could not create context");
            return -1;
        }
        printf("Created context\n");
        
        // 10. prepare graphs
        /* Structure to retrieve information about graphs, like graph name,
        details about input and output tensors preset in libQnnSampleModel.so */
        
        // No. of graphs present in libQnnSampleModel.so
        // true to enable intermediate outputs, false for network outputs only
        bool debug = true;
        QnnLog_Level_t logLevel = QNN_LOG_LEVEL_DEBUG;
        GraphConfigInfo_t **graphConfigsInfo = nullptr;
        if (ModelError_t::MODEL_NO_ERROR !=
                qnnFunctionPointers->composeGraphsFnHandle(backendHandle,
                                                            qnnFunctionPointers->qnnInterface,
                                                            context,
                                                            (const GraphConfigInfo_t**)graphConfigsInfo,
                                                            1,
                                                            &graphsInfo,
                                                            &graphsCount,
                                                            debug,
                                                            logStdoutCallback,
                                                            logLevel)) {
            printf("Failed in composeGraphs()");
            return -1;
        }

        // 11. Finalize graphs
        for (size_t graphIdx = 0; graphIdx < graphsCount; graphIdx++) {
            if (QNN_GRAPH_NO_ERROR != qnnFunctionPointers->qnnInterface.graphFinalize(
                                         (*graphsInfo)[graphIdx].graph, profileBackendHandle, nullptr)) {
                printf("Could not finalize graph");
                return -1;
            }
            /* Extract profiling information if desired and if a valid handle was supplied to finalize
                graphs API */
        }
        // 12. Save context int to a binary file
        // Get the expected size of the buffer from the backend in which the context can be saved
        unsigned long requiredBufferSize{0};
        if (QNN_CONTEXT_NO_ERROR != qnnFunctionPointers->qnnInterface.contextGetBinarySize(context, &requiredBufferSize)) {
            printf("Could not get the required binary size.");
            return -1;
        }
        printf("Required buffer size: %ld bytes\n", requiredBufferSize);
        
        // Allocate a buffer of the required size
        uint8_t* saveBuffer = (uint8_t*)malloc(requiredBufferSize * sizeof(uint8_t));
        if (nullptr == saveBuffer) {
            printf("Could not allocate buffer to save binary.");
            return -1;
        }
        
        auto status = StatusCode::SUCCESS;
        unsigned long writtenBufferSize{0};
        // Pass the allocated buffer and obtain a copy of the context binary written into the buffer
        if (QNN_CONTEXT_NO_ERROR !=
        qnnFunctionPointers->qnnInterface.contextGetBinary(context,
                                                            reinterpret_cast<void*>(saveBuffer),
                                                            requiredBufferSize,
                                                            &writtenBufferSize)) {
            printf("Could not get binary.");
            return -1;
        }
        printf("Written buffer size: %ld bytes\n", writtenBufferSize);
        
        // Check if the supplied buffer size is at least as big as the amount of data witten by the backend
        if (requiredBufferSize < writtenBufferSize) {
            printf("Illegal written buffer size [%ld] bytes. Cannot exceed allocated memory of [%ld] bytes",
                                                            writtenBufferSize,
                                                            requiredBufferSize);
                return -1;
        }
        
        // Use caching utility to save metadata along with the binary buffer from the backend
        const std::string outputPath(cache_model_file);
        std::ofstream os(outputPath, std::ofstream::binary);
        if (!os) {
            printf("Failed to open output file for writing: %s", outputPath.c_str());
            return -1;
        }
        os.write(reinterpret_cast<char*>(saveBuffer), writtenBufferSize);
        os.close();
        printf("Saved binary to file: %s\n", outputPath.c_str());
    } else {
        printf("Loading context from cache file: %s\n", cache_model_file);
        // 13. Load context from a binary file
        auto returnStatus   = StatusCode::SUCCESS;
        
        uint32_t bufferSize = getFileSize(cache_model_file);
        printf("bufferSize: %d\n", bufferSize);

        auto buffer = std::shared_ptr<uint8_t>(new uint8_t[bufferSize], std::default_delete<uint8_t[]>());
        if (!buffer) {
            printf("Failed to allocate memory.");
            return -1;
        }

        std::ifstream in(cachedBinaryPath, std::ifstream::binary);
        if (!in) {
            printf("Failed to open input file: %s\n", cachedBinaryPath.c_str());
            return -1;
        }
        if (!in.read(reinterpret_cast<char*>(buffer.get()), bufferSize)) {
            printf("Failed to read the contents of: %s\n", cachedBinaryPath.c_str());
            return -1;
        }
        printf("Read binary from file: %s\n", cachedBinaryPath.c_str());
        // in.close();

        /* Create a QnnSystemContext handle to access system context APIs. */
        QnnSystemContext_Handle_t sysCtxHandle{nullptr};
        if (QNN_SUCCESS != qnnFunctionPointers->qnnSystemInterface.systemContextCreate(&sysCtxHandle)) {
            printf("Could not create system handle.");
            return -1;
        }
        printf("Created system handle\n");

        /* Retrieve metadata from the context binary through QNN System Context API. */
        const QnnSystemContext_BinaryInfo_t* binaryInfo{nullptr};
        Qnn_ContextBinarySize_t binaryInfoSize{0};
        if (QNN_SUCCESS != qnnFunctionPointers->qnnSystemInterface.systemContextGetBinaryInfo(
                            sysCtxHandle,
                            static_cast<void*>(buffer.get()),
                            bufferSize,
                            &binaryInfo,
                            &binaryInfoSize)) {
            printf("Failed to get context binary info");
            return -1;
        }
        printf("Binary info version: %d\n", binaryInfo->version);
        printf("Binary info size: %ld\n", binaryInfoSize);

        /* Make a copy of the metadata. */
        if (binaryInfo->version == QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_1) {
            if (binaryInfo->contextBinaryInfoV1.graphs) {
                if (!copyGraphsInfo(binaryInfo->contextBinaryInfoV1.graphs,
                                    binaryInfo->contextBinaryInfoV1.numGraphs,
                                    graphsInfo)) {
                    printf("Failed while copying graphs Info.");
                    return -1;
                }
                graphsCount = binaryInfo->contextBinaryInfoV1.numGraphs;
            }
        } else if (binaryInfo->version == QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_2) {
            if (binaryInfo->contextBinaryInfoV2.graphs) {
                if (!copyGraphsInfo(binaryInfo->contextBinaryInfoV2.graphs,
                                    binaryInfo->contextBinaryInfoV2.numGraphs,
                                    graphsInfo)) {
                    printf("Failed while copying graphs Info.");
                    return -1;
                }
                graphsCount = binaryInfo->contextBinaryInfoV2.numGraphs;
            }
        }
        printf("Successfully loaded context from cache file: %s\n", cache_model_file);
        
        /* Release resources associated with previously created QnnSystemContext handle. */
        qnnFunctionPointers->qnnSystemInterface.systemContextFree(sysCtxHandle);
        sysCtxHandle = nullptr;
        
        /* readBuffer contains the binary data that was previously obtained from a backend. Pass this
        cached binary data to the backend to recreate the same context. */
        if (qnnFunctionPointers->qnnInterface.contextCreateFromBinary(backendHandle,
                                                                    deviceHandle,
                                                                    (const QnnContext_Config_t**)&contextConfigs,
                                                                    reinterpret_cast<void*>(buffer.get()),
                                                                    bufferSize,
                                                                    &context,
                                                                    profileBackendHandle)) {
            printf("Could not create context from binary.");
            return -1;
        }
        printf("Successfully created context from cache file: %s\n", cache_model_file);
        
        /* Obtain and save graph handles for each graph present in the context based on the saved graph
        names in the metadata */
        for (size_t graphIdx = 0; graphIdx < graphsCount; graphIdx++) {
            if (QNN_SUCCESS != qnnFunctionPointers->qnnInterface.graphRetrieve(
                    context, (*graphsInfo)[graphIdx].graphName, &((*graphsInfo)[graphIdx].graph))) {
                printf("Unable to retrieve graph handle for graph Idx: %ld", graphIdx);
                return -1;
            }
        }
        printf("Successfully retrieved graph handles from cache file: %s\n", cache_model_file);
    }

    // 打印计算图信息
    for (size_t graphIdx = 0; graphIdx < graphsCount; graphIdx++) {
        printf("Graph Name: %s\n", (*graphsInfo)[graphIdx].graphName);
        printf("Number of input tensors: %d\n", (*graphsInfo)[graphIdx].numInputTensors);
        printf("Number of output tensors: %d\n", (*graphsInfo)[graphIdx].numOutputTensors);
        for (size_t inputIdx = 0; inputIdx < (*graphsInfo)[graphIdx].numInputTensors; inputIdx++) {
            // printf("Input tensor %d: %s\n", inputIdx, (*graphsInfo)[graphIdx].inputTensors[inputIdx].name);
            if ((*graphsInfo)[graphIdx].inputTensors[inputIdx].version == 1) {
                printf("Input tensor %ld: %s, shape: ", inputIdx, (*graphsInfo)[graphIdx].inputTensors[inputIdx].v1.name);
                for (size_t i = 0; i < (*graphsInfo)[graphIdx].inputTensors[inputIdx].v1.rank; i++) {
                    printf("%d ", (*graphsInfo)[graphIdx].inputTensors[inputIdx].v1.dimensions[i]);
                }
                printf("\n");
            } else if ((*graphsInfo)[graphIdx].inputTensors[inputIdx].version == 2) {
                printf("Input tensor %ld: %s, shape: ", inputIdx, (*graphsInfo)[graphIdx].inputTensors[inputIdx].v2.name);
                for (size_t i = 0; i < (*graphsInfo)[graphIdx].inputTensors[inputIdx].v2.rank; i++) {
                    printf("%d ", (*graphsInfo)[graphIdx].inputTensors[inputIdx].v2.dimensions[i]);
                }
                printf("\n");
            }
        }
        for (size_t outputIdx = 0; outputIdx < (*graphsInfo)[graphIdx].numOutputTensors; outputIdx++) {
            if ((*graphsInfo)[graphIdx].outputTensors[outputIdx].version == 1) {
                printf("Output tensor %ld: %s, shape: ", outputIdx, (*graphsInfo)[graphIdx].outputTensors[outputIdx].v1.name);
                for (size_t i = 0; i < (*graphsInfo)[graphIdx].outputTensors[outputIdx].v1.rank; i++) {
                    printf("%d ", (*graphsInfo)[graphIdx].outputTensors[outputIdx].v1.dimensions[i]);
                }
                printf("\n");
            } else if ((*graphsInfo)[graphIdx].outputTensors[outputIdx].version == 2) {
                printf("Output tensor %ld: %s, shape: ", outputIdx, (*graphsInfo)[graphIdx].outputTensors[outputIdx].v2.name);
                for (size_t i = 0; i < (*graphsInfo)[graphIdx].outputTensors[outputIdx].v2.rank; i++) {
                    printf("%d ", (*graphsInfo)[graphIdx].outputTensors[outputIdx].v2.dimensions[i]);
                }
                printf("\n");
            }
        }
    }

    #if 1
    // 14. Execute graphs
    // Select a graph from graphsInfo if there are more than one graph in this context
    uint32_t graphIdx = 0;
    auto graphInfo = (*graphsInfo)[graphIdx];
    uint32_t tensorCount = graphInfo.numInputTensors + graphInfo.numOutputTensors;
    uint32_t inputTensorCount = graphInfo.numInputTensors;
    uint32_t outputTensorCount = graphInfo.numOutputTensors;

    std::vector<int> inputDims;
    std::vector<int> outputDims;
    Qnn_DataType_t input_dataType;
    Qnn_DataType_t output_dataType;
    for (size_t i = 0; i < inputTensorCount; i++) {
        if (graphInfo.inputTensors[i].version == 1) {
            inputDims.insert(inputDims.end(), graphInfo.inputTensors[i].v1.dimensions, graphInfo.inputTensors[i].v1.dimensions + graphInfo.inputTensors[i].v1.rank);
            input_dataType = graphInfo.inputTensors[i].v1.dataType;
        } else if (graphInfo.inputTensors[i].version == 2) {
            inputDims.insert(inputDims.end(), graphInfo.inputTensors[i].v2.dimensions, graphInfo.inputTensors[i].v2.dimensions + graphInfo.inputTensors[i].v2.rank);
            input_dataType = graphInfo.inputTensors[i].v2.dataType;
        }
    }
    for (size_t i = 0; i < outputTensorCount; i++) {
        if (graphInfo.outputTensors[i].version == 1) {
            outputDims.insert(outputDims.end(), graphInfo.outputTensors[i].v1.dimensions, graphInfo.outputTensors[i].v1.dimensions + graphInfo.outputTensors[i].v1.rank);
            output_dataType = graphInfo.outputTensors[i].v1.dataType;
        } else if (graphInfo.outputTensors[i].version == 2) {
            outputDims.insert(outputDims.end(), graphInfo.outputTensors[i].v2.dimensions, graphInfo.outputTensors[i].v2.dimensions + graphInfo.outputTensors[i].v2.rank);
            output_dataType = graphInfo.outputTensors[i].v2.dataType;
        }
    }
    uint32_t inputDataSize = 1;
    if (input_dataType == QNN_DATATYPE_INT_8) {
        inputDataSize = sizeof(uint8_t);
    } else if (input_dataType == QNN_DATATYPE_INT_16) {
        inputDataSize = sizeof(int16_t);
    } else if (input_dataType == QNN_DATATYPE_INT_32) {
        inputDataSize = sizeof(int32_t);
    } else if (input_dataType == QNN_DATATYPE_FLOAT_32) {
        inputDataSize = sizeof(float);
    }   

    uint32_t outputDataSize = 1;
    if (output_dataType == QNN_DATATYPE_INT_8) {
        outputDataSize = sizeof(uint8_t);
    } else if (output_dataType == QNN_DATATYPE_INT_16) {
        outputDataSize = sizeof(int16_t);
    } else if (output_dataType == QNN_DATATYPE_INT_32) {
        outputDataSize = sizeof(int32_t);
    } else if (output_dataType == QNN_DATATYPE_FLOAT_32) {
        outputDataSize = sizeof(float);
    }   

    uint32_t inputSize = 1;
    for (size_t i = 0; i < inputDims.size(); i++) {
        inputSize *= inputDims[i];
    }
    uint32_t outputSize = 1;
    for (size_t i = 0; i < outputDims.size(); i++) {
        outputSize *= outputDims[i];
    }

    Qnn_Tensor_t* input_tensors  =  (Qnn_Tensor_t*)calloc(1, inputTensorCount * sizeof(Qnn_Tensor_t));
    Qnn_Tensor_t* output_tensors =  (Qnn_Tensor_t*)calloc(1, outputTensorCount * sizeof(Qnn_Tensor_t));

    // deepCopyQnnTensorInfo(((*tensors) + tensorIdx), &wrapperTensor)
    for (size_t i = 0; i < inputTensorCount; i++) {
        deepCopyQnnTensorInfo(&input_tensors[i], ((*graphsInfo)[graphIdx].inputTensors + i));
    }

    for (size_t i = 0; i < outputTensorCount; i++) {
        deepCopyQnnTensorInfo(&output_tensors[i], ((*graphsInfo)[graphIdx].outputTensors + i));\
    }

    // 给inputs和outputs分配空间
    for (size_t i = 0; i < inputTensorCount; i++) {
        // QNN_TENSOR_GET_CLIENT_BUF(input_tensors).data = (char *)malloc(inputSize * inputDataSize);
        Qnn_ClientBuffer_t clientBuffer = QNN_CLIENT_BUFFER_INIT;
        clientBuffer.data = (char *)malloc(inputSize * inputDataSize);
        clientBuffer.dataSize = inputSize * inputDataSize;
        QNN_TENSOR_SET_CLIENT_BUF(input_tensors[i], clientBuffer);
    }
    for (size_t i = 0; i < outputTensorCount; i++) {
        // QNN_TENSOR_GET_CLIENT_BUF(output_tensors).data = (char *)malloc(outputSize * outputDataSize);
        Qnn_ClientBuffer_t clientBuffer = QNN_CLIENT_BUFFER_INIT;
        clientBuffer.data = (char *)malloc(outputSize * outputDataSize);
        clientBuffer.dataSize = outputSize * outputDataSize;
        QNN_TENSOR_SET_CLIENT_BUF(output_tensors[i], clientBuffer);
    }

    // 塞数据到inputs
    // 1. 读取输入文件
    std::vector<float> inputData;

    std::ifstream inputFile(input_file, std::ifstream::binary);
    if (!inputFile) {
        printf("Failed to open input file: %s\n", input_file);
        return -1;
    }
    inputFile.seekg(0, std::ios::end);
    size_t inputFileSize = inputFile.tellg();
    inputFile.seekg(0, std::ios::beg);
    inputData.resize(inputFileSize / sizeof(float));
    inputFile.read(reinterpret_cast<char*>(inputData.data()), inputFileSize);
    inputFile.close();
    printf("Read input data from file: %s\n", input_file);

    // 测试,打印inputData
    printf("inputSize: %d, inputDataSize: %d, inputFileSize: %ld\n", inputSize, inputDataSize, inputFileSize);
    // printf("inputData: ")
    // for (size_t i = 0; i < inputSize; i++) {
    //     printf("%f ", inputData[i]);
    // }
    // printf("\n");

    // 2. 塞到inputs
    memcpy((char *)QNN_TENSOR_GET_CLIENT_BUF(input_tensors).data, inputData.data(), inputSize * inputDataSize);
    printf("Copied input data to tensor\n");
    
    // 3. 塞到outputs
    // 4. 执行graph
    Qnn_ErrorHandle_t status = qnnFunctionPointers->qnnInterface.graphExecute(graphInfo.graph,
                                                                              input_tensors,
                                                                              graphInfo.numInputTensors,
                                                                              output_tensors,
                                                                              graphInfo.numOutputTensors,
                                                                              profileBackendHandle,
                                                                              nullptr);
    if (QNN_GRAPH_NO_ERROR != status) {
        printf("Failed to execute graph");
        return -1;
    }
    printf("Successfully executed graph\n");
    // 5. 输出结果到文件
    char* bufferToWrite = reinterpret_cast<char*>(QNN_TENSOR_GET_CLIENT_BUF(output_tensors[0]).data);
    // 保存output_file
    std::ofstream outputFile(output_file, std::ofstream::binary);
    if (!outputFile) {
        printf("Failed to open output file: %s\n", output_file);
        return -1;
    }
    outputFile.write(reinterpret_cast<char*>(bufferToWrite), outputSize * outputDataSize);
    outputFile.close();
    printf("Wrote output data to file: %s\n", output_file);

    // 测试,直接打印data
    // float * test_output_data = reinterpret_cast<float*>(QNN_TENSOR_GET_CLIENT_BUF(output_tensors[0]).data);
    // for (size_t i = 0; i < outputSize; i++) {
    //     printf("%f ", test_output_data[i]);
    // }
    // printf("\n");

    // 15. Free context
    if (QNN_CONTEXT_NO_ERROR !=
          qnnFunctionPointers->qnnInterface.contextFree(context, profileBackendHandle)) {
        printf("Could not free context");
        return -1;
    }
    // 16. terminate backend
    if (QNN_BACKEND_NO_ERROR != qnnFunctionPointers->qnnInterface.backendFree(backendHandle)) {
        printf("Could not free backend");
        return -1;
    }
    #endif
    
    return 0;
}